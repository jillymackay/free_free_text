---
title: "Repeatable"
author: "Jill MacKay"
date: "23/02/2021"
output: 
  html_document:
    highlight: haddock
    theme: flatly
    toc: yes
    toc_depth: 3
    toc_float: yes
---
```{r echo = FALSE, message=FALSE,warning=FALSE}

library(tidyverse)
rawtext <-RCurl::getURL("http://www.gutenberg.org/cache/epub/36774/pg36774.txt") %>% 
  as.tibble() %>% 
  rename(text = value) %>% 
  mutate(text = strsplit(as.character(text), "PROBLEMS OF SCHOOL AND COLLEGE")) %>% 
  unnest(text) %>% 
  mutate(text = strsplit(as.character(text), "HARVARD PAPERS")) %>% 
  unnest(text) %>% 
  mutate(text = strsplit(as.character(text), "PAPERS BY ALICE FREEMAN PALMER")) %>% 
  unnest(text) %>% 
  mutate(text = strsplit(as.character(text), "The Riverside Press")) %>% 
  unnest(text) %>% 
  mutate(collection = c("prelim", "prelim", "prelim", "prelim", "prelim", "PROBLEMS OF SCHOOL AND COLLEGE",
                        "HARVARD PAPERS", "PAPERS BY ALICE FREEMAN PALMER", "prelim")) %>% 
  filter(!collection == "prelim") 
```
# About


# Setting up your R Environment
In this example we will use the following packages


```{r, warning = FALSE, message = FALSE}
library(tidytext)
library(tidyverse)
library(textstem)

```

I also make use of a user-defined function to shorten some steps of the process:


```{r, warning = FALSE, message = FALSE}

# This function allows for quick calculation of Term Frequency - Inverse Document Frequency
lazytf <- function (data, word = "word", grouping_factor) {
  qgv <- enquo (grouping_factor)
  word <- enquo (word)
  data %>%
    group_by(!!qgv) %>%
    count (!!qgv, !!word, sort = TRUE) %>%
    ungroup() %>%
    mutate (total = sum(n)) %>%
    bind_tf_idf (., !!word, !!qgv, n)
  
}



```


# Basic data processing
We have a simple data frame, three 'collections' of free text from [Palmer & Palmer](http://www.gutenberg.org/ebooks/36774) on education. (If you do not have this text you can follow the instructions on how to get it [here](/index.html)). 

This data is currently called `rawdata` in R, and has two columns:
   * `collection` - which essay collection did the row come from
   * `text` - the full text of the essay
   
There are three rows of data, one for each collection.
   
This data therefore follows tidy data principles, in that each row is an observation, and each column is a variable. 

We can process this using the `tidytext` package.


```{r, message = FALSE, warning  = FALSE}

my_stops <- tibble(word = c("gutenberg", "project"),
                   lexicon = c("my_stops", "my_stops"))
my_stops<-full_join(stop_words, my_stops)


freetext <- rawtext %>% 
  unnest_tokens(word, text) %>%
  mutate (lemma = (lemmatize_strings(word))) %>%
  anti_join(my_stops) %>% 
  filter(str_detect(lemma, '[^0-9]'))

```


# Asking simple questions
## What words are used most frequently?

```{r, message = FALSE, warning = FALSE, fig.align = "center"}


freetext %>% 
  count(lemma, sort = T) %>% 
  top_n(20) %>% 
  ggplot(aes(x = reorder(lemma,n), y = n, fill = as.factor(n))) +
  geom_bar(stat = "identity") +
  theme_classic() +
  coord_flip() +
  labs(y = "Count of Word", x = "Word (Lemmatised)",
       title = "Frequency of words across\n'The Teacher: Essays and Addresses on Education by Palmer and Palmer'",
       subtitle = "Words are lemmatised") +
  theme(legend.position = "none")



```


## What words are used most frequently within each collection?
```{r, message = FALSE, warning = FALSE, fig.align = "center"}

freetext %>% 
  count(collection, lemma, sort = T) %>% 
  top_n(20) %>% 
  ggplot(aes(x = reorder(lemma,n), y = n, fill = as.factor(n))) +
  geom_bar(stat = "identity") +
  theme_classic() +
  coord_flip() +
  facet_wrap(facets = ~ collection, scales = "free", nrow = 3) +
  labs(y = "Count of Word", x = "Word (Lemmatised)",
       title = "Frequency of words across\n'The Teacher: Essays and Addresses on Education by Palmer and Palmer'",
       subtitle = "Words are lemmatised") +
  theme(legend.position = "none")

```


# Are some words more unique in certain collections?
## Calculating Term Frequency - Inverse Document Frequency (TF-IDF)
```{r, message = FALSE, warning = FALSE}
freetf <- lazytf(freetext, word = lemma, grouping_factor = collection)


```


## Visualising TF-IDF
```{r, message = FALSE, warning = FALSE, fig.align = "center"}
freetf %>% 
  arrange(desc (tf_idf)) %>%
  mutate (lemma = factor (lemma, levels = rev(unique(lemma)))) %>%
  group_by (collection) %>%
  top_n(7, tf_idf) %>%
  ungroup()  %>% 
  ggplot(aes(lemma, tf_idf, fill = lemma)) +
  geom_col(show.legend = FALSE) +
  labs (x = NULL, y = "Term Frequency - Inverse Document (Question) Frequency",
        title = "Relative uniqueness of a term's frequency in each collection") +
  facet_wrap(~collection, nrow = 3, scales = "free") +
  theme_classic() +
  coord_flip() 

```



# Are there differences in sentiment across the collections?
```{r, message=FALSE, warning = FALSE, fig.align= "center"}

freetext %>% 
  inner_join(get_sentiments("bing")) %>%
  count(collection, lemma, sentiment, sort = TRUE) %>%
  ungroup() %>% 
  group_by(sentiment) %>%
  top_n(50) %>%
  ungroup() %>%
  mutate(lemma = reorder(lemma, n)) %>%
  ggplot(aes(n, lemma, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~collection + sentiment, scales = "free_y", nrow = 3) +
  labs(x = "Contribution to sentiment",
       y = NULL)






freetext %>% 
  mutate(location = row_number(),
         word= lemma) %>% 
  inner_join(get_sentiments("bing")) %>%
  count(collection, index = location %/% 50, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>% 
  ggplot(aes(index, sentiment, fill = collection)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~collection, ncol = 1, scales = "free_x")




freetext %>% 
  mutate(location = row_number(),
         word = lemma) %>% 
  inner_join(get_sentiments("afinn")) %>% 
  ggplot(aes(x = location, y= value, fill = collection))+
  geom_col(show.legend = FALSE)  +
  facet_wrap(facets = ~collection, ncol = 1, scales = "free_x")

```

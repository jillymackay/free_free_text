---
title: "Repeatable"
author: "Jill MacKay"
date: "`r Sys.Date()`"
output: 
  html_document:
    highlight: haddock
    theme: flatly
    toc: yes
    toc_depth: 3
    toc_float: yes
---
```{r echo = FALSE, message=FALSE,warning=FALSE}

library(tidyverse)
rawtext <-RCurl::getURL("http://www.gutenberg.org/cache/epub/36774/pg36774.txt") %>% 
  as.tibble() %>% 
  rename(text = value) %>% 
  mutate(text = strsplit(as.character(text), "PROBLEMS OF SCHOOL AND COLLEGE")) %>% 
  unnest(text) %>% 
  mutate(text = strsplit(as.character(text), "HARVARD PAPERS")) %>% 
  unnest(text) %>% 
  mutate(text = strsplit(as.character(text), "PAPERS BY ALICE FREEMAN PALMER")) %>% 
  unnest(text) %>% 
  mutate(text = strsplit(as.character(text), "The Riverside Press")) %>% 
  unnest(text) %>% 
  mutate(collection = c("prelim", "prelim", "prelim", "prelim", "prelim", "PROBLEMS OF SCHOOL AND COLLEGE",
                        "HARVARD PAPERS", "PAPERS BY ALICE FREEMAN PALMER", "prelim")) %>% 
  filter(!collection == "prelim") 
```
# About
This file accompanies MacKay 2021 "Freeing the Free Text Comment". It shows a reproducible and shareable workflow for text mining where the data itself may not be shareable. The sample data I am using is [Palmer & Herbert's The Teacher](http://www.gutenberg.org/ebooks/36774). (Although you can utilise this process on any text, you can follow the [instructions here](index.html) on how to get this specific text). 


# Setting up your R Environment
In this example we will use the following packages


```{r, warning = FALSE, message = FALSE}
library(tidytext)
library(tidyverse)
library(textstem)

```

I also make use of a user-defined function to shorten some steps of the process:


```{r, warning = FALSE, message = FALSE}

# This function allows for quick calculation of Term Frequency - Inverse Document Frequency
lazytf <- function (data, word = "word", grouping_factor) {
  qgv <- enquo (grouping_factor)
  word <- enquo (word)
  data %>%
    group_by(!!qgv) %>%
    count (!!qgv, !!word, sort = TRUE) %>%
    ungroup() %>%
    mutate (total = sum(n)) %>%
    bind_tf_idf (., !!word, !!qgv, n)
  
}

```


# Documenting data files
Our current data frame is a data frame currently called `rawdata`. The data frame has two columns:

   * `collection` - which essay collection did the row come from

   * `text` - the full text of the essay
   
The data frame has three rows of data, one for each collection.
   
This data therefore follows tidy data principles, in that each row is an observation, and each column is a variable. 

We can process this using the `tidytext` package.


The following code fully describes (in an automated fashion) how the data is processed ready for text mining. 

```{r, message = FALSE, warning  = FALSE}

# Create a lexicon of words which should not be considered
# And append this custom lexicon to the standard 'stop_words' lexicon

my_stops <- tibble(word = c("gutenberg", "project"),
                   lexicon = c("my_stops", "my_stops"))
my_stops <- full_join(stop_words, my_stops)


# Tokenise the text
# Run a lemmatisation protocol
# Remove common words based on custom lexicon
# Remove any remaining words which are only symbols

freetext <- rawtext %>% 
  unnest_tokens(word, text) %>%
  mutate (lemma = (lemmatize_strings(word))) %>%
  anti_join(my_stops) %>% 
  filter(str_detect(lemma, '[^0-9]'))

```


# Documenting all operations
## What words are used most frequently?

```{r, message = FALSE, warning = FALSE, fig.align = "center"}


freetext %>% 
  count(lemma, sort = T) %>% 
  top_n(20) %>% 
  ggplot(aes(x = reorder(lemma,n), y = n, fill = as.factor(n))) +
  geom_bar(stat = "identity") +
  theme_classic() +
  coord_flip() +
  labs(y = "Count of Word", x = "Word (Lemmatised)",
       title = "Frequency of words across\n'The Teacher: Essays and Addresses on Education by Palmer and Herbert'",
       subtitle = "Words are lemmatised") +
  theme(legend.position = "none")



```


## What words are used most frequently within each collection?
```{r, message = FALSE, warning = FALSE, fig.align = "center"}

freetext %>% 
  count(collection, lemma, sort = T) %>% 
  top_n(20) %>% 
  ggplot(aes(x = reorder(lemma,n), y = n, fill = as.factor(n))) +
  geom_bar(stat = "identity") +
  theme_classic() +
  coord_flip() +
  facet_wrap(facets = ~ collection, scales = "free", nrow = 3) +
  labs(y = "Count of Word", x = "Word (Lemmatised)",
       title = "Frequency of words across\n'The Teacher: Essays and Addresses on Education by Palmer and Herbert'",
       subtitle = "Words are lemmatised") +
  theme(legend.position = "none")

```


## Are some words more unique in certain collections?
### Calculating Term Frequency - Inverse Document Frequency (TF-IDF)
```{r, message = FALSE, warning = FALSE}
freetf <- lazytf(freetext, word = lemma, grouping_factor = collection)


```


### Visualising TF-IDF
```{r, message = FALSE, warning = FALSE, fig.align = "center"}
freetf %>% 
  arrange(desc (tf_idf)) %>%
  mutate (lemma = factor (lemma, levels = rev(unique(lemma)))) %>%
  group_by (collection) %>%
  top_n(7, tf_idf) %>%
  ungroup()  %>% 
  ggplot(aes(lemma, tf_idf, fill = lemma)) +
  geom_col(show.legend = FALSE) +
  labs (x = NULL, y = "Term Frequency - Inverse Document (Collection) Frequency",
        title = "Relative uniqueness of a term's frequency in each collection") +
  facet_wrap(~collection, nrow = 3, scales = "free") +
  theme_classic() +
  coord_flip() 

```



## Are there differences in sentiment across the collections?
```{r, message=FALSE, warning = FALSE, fig.align= "center"}

freetext %>% 
  inner_join(get_sentiments("bing")) %>%
  count(collection, lemma, sentiment, sort = TRUE) %>%
  ungroup() %>% 
  group_by(sentiment) %>%
  top_n(50) %>%
  ungroup() %>%
  mutate(lemma = reorder(lemma, n)) %>%
  ggplot(aes(n, lemma, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~collection + sentiment, scales = "free_y", nrow = 3) +
  labs(x = "Contribution to sentiment",
       y = NULL)






freetext %>% 
  mutate(location = row_number(),
         word= lemma) %>% 
  inner_join(get_sentiments("bing")) %>%
  count(collection, index = location %/% 50, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>% 
  ggplot(aes(index, sentiment, fill = collection)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~collection, ncol = 1, scales = "free_x")




freetext %>% 
  mutate(location = row_number(),
         word = lemma) %>% 
  inner_join(get_sentiments("afinn")) %>% 
  ggplot(aes(x = location, y= value, fill = collection))+
  geom_col(show.legend = FALSE)  +
  facet_wrap(facets = ~collection, ncol = 1, scales = "free_x")

```



